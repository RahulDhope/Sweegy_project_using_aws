

-- Create a warehouse if not exist
create warehouse if not exists adhoc_wh
      comment = 'This is the adhoc-wh'
      warehouse_size = 'x-small'
      auto_resume = true
      auto_suspend = 60
      enable_query_acceleration = false
      warehouse_type = 'standard'
      min_cluster_count = 1
      max_cluster_count = 1
      scaling_policy = 'standard'
      initially_suspended = true;

-- create development database/schema if does not exist
create database if not exists sandbox;
use database sandbox;
create schema if not exists stage_sch;
create schema if not exists clean_sch;
create schema if not exists consumption_sch;
create schema if not exists common;

use schema stage_sch;

-- create file format to process the CSV file
create file format if not exists stage_sch.csv_file_format
         type = 'csv'
         compression = 'auto'
         field_delimiter = ','
         record_delimiter = '\n'
         skip_header = 1
         field_optionally_enclosed_by = '\042'
         null_if = ('\\N');

create stage stage_sch.csv_stg
    directory = ( enable = true )
    comment = 'this is the snowflake internal stage';


-- Data Governance - Tags and Masking Policies (Placeholders)
create or replace tag
    common.pii_policy_tag
    allowed_values 'PII','PRICE','SENSITIVE','EMAIL'
    comment = 'This is PII policy tag object';

create or replace masking policy
    common.pii_masking_policy as (pii_text string)
    returns string ->
    to_varchar('** PII **');

create or replace masking policy
    common.email_masking_policy as (email_text string)
    returns string ->
    to_varchar('** EAMIL **');

create or replace masking policy
    common.phone_masking_policy as (phone string)
    returns string ->
    to_varchar('** Phone **');
-- ---------------------------------------------------------------------------------------------------------------------


-- 2. Location Dimension Data Pipeline (SCD Type 1 - Update-in-Place)
-- ---------------------------------------------------------------------------------------------------------------------

-- STAGE LAYER (Location)
create or replace table stage_sch.location (
    locationid text, city text, state text, zipcode text, activeflag text,
    createddate text, modifieddate text,
    _stg_file_name text, _stg_file_load_ts timestamp, _stg_file_md5 text,
    _copy_data_ts timestamp default current_timestamp
)
comment = 'Location stage/raw table.';

create or replace stream stage_sch.location_stm
on table stage_sch.location
append_only = true
comment = 'Append-only stream on location table.';


-- Initial Load (Location)
copy into stage_sch.location (locationid, city, state, zipcode, activeflag,
                          createddate, modifieddate, _stg_file_name,
                          _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as locationid, t.$2::text as city, t.$3::text as state,
        t.$4::text as zipcode, t.$5::text as activeflag, t.$6::text as createddate,
        t.$7::text as modifieddate, metadata$filename as _stg_file_name,
        metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5,
        current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/initial/location t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;


-- CLEAN LAYER (Restaurant Location)
use schema clean_sch;
create or replace table clean_sch.restaurant_location (
    restaurant_location_sk number autoincrement primary key,
    location_id number not null unique,
    city string(100) not null, state string(100) not null,
    state_code string(2) not null, is_union_territory boolean not null default false,
    capital_city_flag boolean not null default false, city_tier text(6),
    zip_code string(10) not null, active_flag string(10) not null,
    created_ts timestamp_tz not null, modified_ts timestamp_tz,
    _stg_file_name string, _stg_file_load_ts timestamp_ntz,
    _stg_file_md5 string, _copy_data_ts timestamp_ntz default current_timestamp
)
comment = 'Location entity under clean schema with appropriate data type (SCD1).';

create or replace stream clean_sch.restaurant_location_stm
on table clean_sch.restaurant_location
comment = 'Standard stream on location table to track changes.';


-- Merge from Stage to Clean (Location)
MERGE INTO clean_sch.restaurant_location AS target
USING (
    SELECT
        CAST(LocationID AS NUMBER) AS Location_ID, CAST(City AS STRING) AS City,
        CASE
            WHEN CAST(State AS STRING) = 'Delhi' THEN 'New Delhi'
            ELSE CAST(State AS STRING)
        END AS State,
        CASE
            WHEN State = 'Delhi' THEN 'DL' WHEN State = 'Maharashtra' THEN 'MH'
            WHEN State = 'Uttar Pradesh' THEN 'UP' WHEN State = 'Gujarat' THEN 'GJ'
            WHEN State = 'Rajasthan' THEN 'RJ' WHEN State = 'Kerala' THEN 'KL'
            WHEN State = 'Punjab' THEN 'PB' WHEN State = 'Karnataka' THEN 'KA'
            WHEN State = 'Madhya Pradesh' THEN 'MP' WHEN State = 'Odisha' THEN 'OR'
            WHEN State = 'Chandigarh' THEN 'CH' WHEN State = 'West Bengal' THEN 'WB'
            WHEN State = 'Sikkim' THEN 'SK' WHEN State = 'Andhra Pradesh' THEN 'AP'
            WHEN State = 'Assam' THEN 'AS' WHEN State = 'Jammu and Kashmir' THEN 'JK'
            WHEN State = 'Puducherry' THEN 'PY' WHEN State = 'Uttarakhand' THEN 'UK'
            WHEN State = 'Himachal Pradesh' THEN 'HP' WHEN State = 'Tamil Nadu' THEN 'TN'
            WHEN State = 'Goa' THEN 'GA' WHEN State = 'Telangana' THEN 'TG'
            WHEN State = 'Chhattisgarh' THEN 'CG' WHEN State = 'Jharkhand' THEN 'JH'
            WHEN State = 'Bihar' THEN 'BR'
            ELSE NULL
        END AS state_code,
        CASE
            WHEN State IN ('Delhi', 'Chandigarh', 'Puducherry', 'Jammu and Kashmir') THEN TRUE
            ELSE FALSE
        END AS is_union_territory,
        CASE
            WHEN (State = 'Delhi' AND City = 'New Delhi') THEN TRUE
            WHEN (State = 'Maharashtra' AND City = 'Mumbai') THEN TRUE
            ELSE FALSE
        END AS capital_city_flag,
        CASE
            WHEN City IN ('Mumbai', 'Delhi', 'Bengaluru', 'Hyderabad', 'Chennai', 'Kolkata', 'Pune', 'Ahmedabad') THEN 'Tier-1'
            WHEN City IN ('Jaipur', 'Lucknow', 'Kanpur', 'Nagpur', 'Indore', 'Bhopal', 'Patna', 'Vadodara', 'Coimbatore',
                          'Ludhiana', 'Agra', 'Nashik', 'Ranchi', 'Meerut', 'Raipur', 'Guwahati', 'Chandigarh') THEN 'Tier-2'
            ELSE 'Tier-3'
        END AS city_tier,
        CAST(ZipCode AS STRING) AS Zip_Code,
        CAST(ActiveFlag AS STRING) AS Active_Flag,
        TO_TIMESTAMP_TZ(CreatedDate, 'YYYY-MM-DD HH24:MI:SS') AS created_ts,
        TO_TIMESTAMP_TZ(ModifiedDate, 'YYYY-MM-DD HH24:MI:SS') AS modified_ts,
        _stg_file_name, _stg_file_load_ts, _stg_file_md5, CURRENT_TIMESTAMP AS _copy_data_ts
    FROM stage_sch.location_stm
) AS source
ON target.Location_ID = source.Location_ID
WHEN MATCHED AND (
    target.City != source.City OR target.State != source.State OR
    target.state_code != source.state_code OR target.is_union_territory != source.is_union_territory OR
    target.capital_city_flag != source.capital_city_flag OR target.city_tier != source.city_tier OR
    target.Zip_Code != source.Zip_Code OR target.Active_Flag != source.Active_Flag OR
    target.modified_ts != source.modified_ts
) THEN
    UPDATE SET
        target.City = source.City, target.State = source.State,
        target.state_code = source.state_code, target.is_union_territory = source.is_union_territory,
        target.capital_city_flag = source.capital_city_flag, target.city_tier = source.city_tier,
        target.Zip_Code = source.Zip_Code, target.Active_Flag = source.Active_Flag,
        target.modified_ts = source.modified_ts, target._stg_file_name = source._stg_file_name,
        target._stg_file_load_ts = source._stg_file_load_ts, target._stg_file_md5 = source._stg_file_md5,
        target._copy_data_ts = source._copy_data_ts
WHEN NOT MATCHED THEN
    INSERT (Location_ID, City, State, state_code, is_union_territory, capital_city_flag, city_tier, Zip_Code, Active_Flag, created_ts, modified_ts, _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
    VALUES (source.Location_ID, source.City, source.State, source.state_code, source.is_union_territory, source.capital_city_flag, source.city_tier, source.Zip_Code, source.Active_Flag, source.created_ts, source.modified_ts, source._stg_file_name, source._stg_file_load_ts, source._stg_file_md5, source._copy_data_ts);

-- CONSUMPTION LAYER (Restaurant Location Dim - SCD Type 1)
use schema consumption_sch;
create or replace table consumption_sch.restaurant_location_dim (
    restaurant_location_sk number autoincrement primary key,
    location_id number(38,0) not null unique,
    city varchar(100) not null, state varchar(100) not null,
    state_code varchar(2) not null, is_union_territory boolean not null default false,
    capital_city_flag boolean not null default false, city_tier varchar(6),
    zip_code varchar(10) not null, active_flag varchar(10) not null,
    created_ts timestamp_tz, modified_ts timestamp_tz,
    restaurant_location_hk number -- Added for consistent HK usage, although not strictly needed for SCD1
)
comment = 'Location Dimension table with SCD1 for faster query performance.';


-- Merge from Clean Stream to Consumption Dim (Location - SCD Type 1 Logic)
MERGE INTO consumption_sch.RESTAURANT_LOCATION_DIM AS target
USING (
    SELECT
        source.Location_ID, source.City, source.State, source.state_code, source.is_union_territory,
        source.capital_city_flag, source.city_tier, source.Zip_Code, source.Active_Flag,
        source.created_ts, source.modified_ts,
        hash(SHA1_hex(CAST(source.LOCATION_ID AS VARCHAR) || source.CITY || source.STATE || source.STATE_CODE || source.ZIP_CODE)) AS restaurant_location_hk
    FROM clean_sch.RESTAURANT_LOCATION_STM AS source
    WHERE source.METADATA$ACTION = 'INSERT' OR source.METADATA$ISUPDATE = 'TRUE'
) AS source
ON target.Location_ID = source.Location_ID
WHEN MATCHED THEN
    -- Update the current record (SCD Type 1) if any attributes change
    UPDATE SET
        target.City = source.City, target.State = source.State, target.state_code = source.state_code,
        target.is_union_territory = source.is_union_territory, target.capital_city_flag = source.capital_city_flag,
        target.city_tier = source.city_tier, target.Zip_Code = source.Zip_Code, target.Active_Flag = source.Active_Flag,
        target.modified_ts = source.modified_ts,
        target.restaurant_location_hk = source.restaurant_location_hk -- Update the hash key too
WHEN NOT MATCHED THEN
    -- Insert new record
    INSERT (
        LOCATION_ID, CITY, STATE, STATE_CODE, IS_UNION_TERRITORY,
        CAPITAL_CITY_FLAG, CITY_TIER, ZIP_CODE, ACTIVE_FLAG,
        CREATED_TS, MODIFIED_TS, restaurant_location_hk
    )
    VALUES (
        source.LOCATION_ID, source.CITY, source.STATE, source.state_code, source.is_union_territory,
        source.capital_city_flag, source.city_tier, source.Zip_Code, source.Active_Flag,
        source.created_ts, source.modified_ts, source.restaurant_location_hk
    );

-- Delta Load (Location)
use schema stage_sch;
copy into stage_sch.location (locationid, city, state, zipcode, activeflag,
                          createddate, modifieddate, _stg_file_name,
                          _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as locationid, t.$2::text as city, t.$3::text as state,
        t.$4::text as zipcode, t.$5::text as activeflag, t.$6::text as createddate,
        t.$7::text as modifieddate, metadata$filename as _stg_file_name,
        metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5,
        current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/delta/location/delta-day02-2rows-update.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
-- ---------------------------------------------------------------------------------------------------------------------


-- 3. Restaurant Dimension Data Pipeline (SCD Type 2)
-- ---------------------------------------------------------------------------------------------------------------------

-- STAGE LAYER (Restaurant)
use schema stage_sch;
create or replace table stage_sch.restaurant (
    restaurantid text, name text, cuisinetype text, pricing_for_2 text,
    restaurant_phone text WITH TAG (common.pii_policy_tag = 'SENSITIVE'),
    operatinghours text, locationid text, activeflag text, openstatus text,
    locality text, restaurant_address text, latitude text, longitude text,
    createddate text, modifieddate text,
    _stg_file_name text, _stg_file_load_ts timestamp, _stg_file_md5 text,
    _copy_data_ts timestamp default current_timestamp
)
comment = 'Restaurant stage/raw table.';

create or replace stream stage_sch.restaurant_stm
on table stage_sch.restaurant
append_only = true
comment = 'Append-only stream on restaurant table.';

-- Initial Load (Restaurant)
copy into stage_sch.restaurant (restaurantid, name, cuisinetype, pricing_for_2, restaurant_phone,
                              operatinghours, locationid, activeflag, openstatus,
                              locality, restaurant_address, latitude, longitude,
                              createddate, modifieddate,
                              _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as restaurantid, t.$2::text as name, t.$3::text as cuisinetype,
        t.$4::text as pricing_for_2, t.$5::text as restaurant_phone, t.$6::text as operatinghours,
        t.$7::text as locationid, t.$8::text as activeflag, t.$9::text as openstatus,
        t.$10::text as locality, t.$11::text as restaurant_address, t.$12::text as latitude,
        t.$13::text as longitude, t.$14::text as createddate, t.$15::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp() as _copy_data_ts
      from @stage_sch.csv_stg/initial/restaurant/restaurant-delhi+NCR.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;


-- CLEAN LAYER (Restaurant)
use schema clean_sch;
create or replace table clean_sch.restaurant (
    restaurant_sk number autoincrement primary key,
    restaurant_id number unique,
    name string(100) not null, cuisine_type string, pricing_for_two number(10, 2),
    restaurant_phone string(15) WITH TAG (common.pii_policy_tag = 'SENSITIVE'),
    operating_hours string(100), location_id_fk number,
    active_flag string(10), open_status string(10), locality string(100),
    restaurant_address string, latitude number(9, 6), longitude number(9, 6),
    created_dt timestamp_tz, modified_dt timestamp_tz,
    _stg_file_name string, _stg_file_load_ts timestamp_ntz,
    _stg_file_md5 string, _copy_data_ts timestamp_ntz default current_timestamp
)
comment = 'Restaurant entity under clean schema with appropriate data type (SCD1).';

create or replace stream clean_sch.restaurant_stm
on table clean_sch.restaurant
comment = 'Standard stream on clean restaurant table to track changes.';

-- Merge from Stage to Clean (Restaurant)
MERGE INTO clean_sch.restaurant AS target
USING (
    SELECT
        try_cast(restaurantid AS number) AS restaurant_id, try_cast(name AS string) AS name,
        try_cast(cuisinetype AS string) AS cuisine_type, try_cast(pricing_for_2 AS number(10, 2)) AS pricing_for_two,
        try_cast(restaurant_phone AS string) AS restaurant_phone, try_cast(operatinghours AS string) AS operating_hours,
        try_cast(locationid AS number) AS location_id_fk, try_cast(activeflag AS string) AS active_flag,
        try_cast(openstatus AS string) AS open_status, try_cast(locality AS string) AS locality,
        try_cast(restaurant_address AS string) AS restaurant_address, try_cast(latitude AS number(9, 6)) AS latitude,
        try_cast(longitude AS number(9, 6)) AS longitude,
        try_to_timestamp_ntz(createddate, 'YYYY-MM-DD HH24:MI:SS.FF9') AS created_dt,
        try_to_timestamp_ntz(modifieddate, 'YYYY-MM-DD HH24:MI:SS.FF9') AS modified_dt,
        _stg_file_name, _stg_file_load_ts, _stg_file_md5
    FROM
        stage_sch.restaurant_stm
) AS source
ON target.restaurant_id = source.restaurant_id
WHEN MATCHED THEN
    UPDATE SET
        target.name = source.name, target.cuisine_type = source.cuisine_type,
        target.pricing_for_two = source.pricing_for_two, target.restaurant_phone = source.restaurant_phone,
        target.operating_hours = source.operating_hours, target.location_id_fk = source.location_id_fk,
        target.active_flag = source.active_flag, target.open_status = source.open_status,
        target.locality = source.locality, target.restaurant_address = source.restaurant_address,
        target.latitude = source.latitude, target.longitude = source.longitude,
        target.created_dt = source.created_dt, target.modified_dt = source.modified_dt,
        target._stg_file_name = source._stg_file_name, target._stg_file_load_ts = source._stg_file_load_ts,
        target._stg_file_md5 = source._stg_file_md5
WHEN NOT MATCHED THEN
    INSERT (
        restaurant_id, name, cuisine_type, pricing_for_two, restaurant_phone,
        operating_hours, location_id_fk, active_flag, open_status, locality,
        restaurant_address, latitude, longitude, created_dt, modified_dt,
        _stg_file_name, _stg_file_load_ts, _stg_file_md5
    )
    VALUES (
        source.restaurant_id, source.name, source.cuisine_type, source.pricing_for_two,
        source.restaurant_phone, source.operating_hours, source.location_id_fk,
        source.active_flag, source.open_status, source.locality,
        source.restaurant_address, source.latitude, source.longitude,
        source.created_dt, source.modified_dt,
        source._stg_file_name, source._stg_file_load_ts, source._stg_file_md5
    );


-- CONSUMPTION LAYER (Restaurant Dim - SCD Type 2)
use schema consumption_sch;
CREATE OR REPLACE TABLE CONSUMPTION_SCH.RESTAURANT_DIM (
    RESTAURANT_HK NUMBER primary key,
    RESTAURANT_ID NUMBER,
    NAME STRING(100), CUISINE_TYPE STRING, PRICING_FOR_TWO NUMBER(10, 2),
    RESTAURANT_PHONE STRING(15) WITH TAG (common.pii_policy_tag = 'SENSITIVE'),
    OPERATING_HOURS STRING(100), LOCATION_ID_FK NUMBER, ACTIVE_FLAG STRING(10),
    OPEN_STATUS STRING(10), LOCALITY STRING(100), RESTAURANT_ADDRESS STRING,
    LATITUDE NUMBER(9, 6), LONGITUDE NUMBER(9, 6),
    EFF_START_DATE TIMESTAMP_TZ, EFF_END_DATE TIMESTAMP_TZ, IS_CURRENT BOOLEAN
)
COMMENT = 'Dimensional table for Restaurant entity with hash keys and SCD2 enabled.';


-- Merge from Clean Stream to Consumption Dim (Restaurant - SCD Type 2 Logic)
MERGE INTO
    CONSUMPTION_SCH.RESTAURANT_DIM AS target
USING
    CLEAN_SCH.RESTAURANT_STM AS source
ON
    target.RESTAURANT_ID = source.RESTAURANT_ID AND
    target.IS_CURRENT = TRUE -- Only check against the current record
WHEN MATCHED
    AND source.METADATA$ACTION = 'UPDATE' THEN
    -- A change occurred in the source, so expire the current dimension record (SCD Type 2)
    UPDATE SET
        target.EFF_END_DATE = CURRENT_TIMESTAMP(),
        target.IS_CURRENT = FALSE
WHEN NOT MATCHED
    AND source.METADATA$ACTION = 'INSERT' THEN
    -- Insert the new or changed record (new version or a truly new record)
    INSERT (
        RESTAURANT_HK, RESTAURANT_ID, NAME, CUISINE_TYPE, PRICING_FOR_TWO,
        RESTAURANT_PHONE, OPERATING_HOURS, LOCATION_ID_FK, ACTIVE_FLAG, OPEN_STATUS,
        LOCALITY, RESTAURANT_ADDRESS, LATITUDE, LONGITUDE,
        EFF_START_DATE, EFF_END_DATE, IS_CURRENT
    )
    VALUES (
        hash(SHA1_hex(CONCAT(source.RESTAURANT_ID, source.NAME, source.CUISINE_TYPE,
            source.PRICING_FOR_TWO, source.RESTAURANT_PHONE, source.OPERATING_HOURS,
            source.LOCATION_ID_FK, source.ACTIVE_FLAG, source.OPEN_STATUS, source.LOCALITY,
            source.RESTAURANT_ADDRESS, source.LATITUDE, source.LONGITUDE))),
        source.RESTAURANT_ID, source.NAME, source.CUISINE_TYPE,
        source.PRICING_FOR_TWO, source.RESTAURANT_PHONE, source.OPERATING_HOURS,
        source.LOCATION_ID_FK, source.ACTIVE_FLAG, source.OPEN_STATUS,
        source.LOCALITY, source.RESTAURANT_ADDRESS, source.LATITUDE,
        source.LONGITUDE, CURRENT_TIMESTAMP(), NULL, TRUE
    );


-- Delta Load (Restaurant)
use schema stage_sch;
copy into stage_sch.restaurant (restaurantid, name, cuisinetype, pricing_for_2, restaurant_phone,
                              operatinghours, locationid, activeflag, openstatus,
                              locality, restaurant_address, latitude, longitude,
                              createddate, modifieddate,
                              _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as restaurantid, t.$2::text as name, t.$3::text as cuisinetype,
        t.$4::text as pricing_for_2, t.$5::text as restaurant_phone, t.$6::text as operatinghours,
        t.$7::text as locationid, t.$8::text as activeflag, t.$9::text as openstatus,
        t.$10::text as locality, t.$11::text as restaurant_address, t.$12::text as latitude,
        t.$13::text as longitude, t.$14::text as createddate, t.$15::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp() as _copy_data_ts
      from @stage_sch.csv_stg/delta/restaurant/day-02-upsert-restaurant-delhi+NCR.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
-- ---------------------------------------------------------------------------------------------------------------------


-- 4. Customer Dimension Data Pipeline (SCD Type 2)
-- ---------------------------------------------------------------------------------------------------------------------

-- STAGE LAYER (Customer)
use schema stage_sch;
create or replace table stage_sch.customer (
    customerid text, name text,
    mobile text WITH TAG (common.pii_policy_tag = 'PII'),
    email text WITH TAG (common.pii_policy_tag = 'EMAIL'),
    loginbyusing text,
    gender text WITH TAG (common.pii_policy_tag = 'PII'),
    dob text WITH TAG (common.pii_policy_tag = 'PII'),
    anniversary text, preferences text, createddate text, modifieddate text,
    _stg_file_name text, _stg_file_load_ts timestamp, _stg_file_md5 text,
    _copy_data_ts timestamp default current_timestamp
)
comment = 'Customer stage/raw table.';

create or replace stream stage_sch.customer_stm
on table stage_sch.customer
append_only = true
comment = 'Append-only stream on customer table.';

-- Initial Load (Customer)
copy into stage_sch.customer (customerid, name, mobile, email, loginbyusing, gender, dob, anniversary,
                          preferences, createddate, modifieddate,
                          _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as customerid, t.$2::text as name, t.$3::text as mobile,
        t.$4::text as email, t.$5::text as loginbyusing, t.$6::text as gender,
        t.$7::text as dob, t.$8::text as anniversary, t.$9::text as preferences,
        t.$10::text as createddate, t.$11::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/initial/customer/customers-initial.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;


-- CLEAN LAYER (Customer)
use schema clean_sch;
CREATE OR REPLACE TABLE CLEAN_SCH.CUSTOMER (
    CUSTOMER_SK NUMBER AUTOINCREMENT PRIMARY KEY,
    CUSTOMER_ID STRING NOT NULL UNIQUE,
    NAME STRING(100) NOT NULL,
    MOBILE STRING(15) WITH TAG (common.pii_policy_tag = 'PII'),
    EMAIL STRING(100) WITH TAG (common.pii_policy_tag = 'EMAIL'),
    LOGIN_BY_USING STRING(50), GENDER STRING(10) WITH TAG (common.pii_policy_tag = 'PII'),
    DOB DATE WITH TAG (common.pii_policy_tag = 'PII'), ANNIVERSARY DATE, PREFERENCES STRING,
    CREATED_DT TIMESTAMP_TZ DEFAULT CURRENT_TIMESTAMP, MODIFIED_DT TIMESTAMP_TZ,
    _STG_FILE_NAME STRING, _STG_FILE_LOAD_TS TIMESTAMP_NTZ,
    _STG_FILE_MD5 STRING, _COPY_DATA_TS TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
)
comment = 'Customer entity under clean schema with appropriate data type (SCD1).';

create or replace stream CLEAN_SCH.customer_stm
on table CLEAN_SCH.customer
comment = 'Standard stream on clean customer table to track changes.';

-- Initial Insert from Stage to Clean (Customer) - Important for initial load to capture all base records.
INSERT INTO clean_sch.customer (
    customer_id, name, mobile, email, login_by_using, gender, dob, anniversary,
    preferences, created_dt, modified_dt, _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts
)
SELECT
    customerid::string, name::string, mobile::string, email::string,
    loginbyusing::string, gender::string, try_to_date(dob, 'YYYY-MM-DD') as dob,
    try_to_date(anniversary, 'YYYY-MM-DD') as anniversary, preferences::string,
    try_to_timestamp_tz(createddate, 'YYYY-MM-DD HH24:MI:SS') as created_dt,
    try_to_timestamp_tz(modifieddate, 'YYYY-MM-DD HH24:MI:SS') as modified_dt,
    _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts
FROM stage_sch.customer;


-- Merge from Stage Stream to Clean (Customer)
MERGE INTO CLEAN_SCH.CUSTOMER AS target
USING (
    SELECT
        CUSTOMERID::STRING AS CUSTOMER_ID, NAME::STRING AS NAME, MOBILE::STRING AS MOBILE,
        EMAIL::STRING AS EMAIL, LOGINBYUSING::STRING AS LOGIN_BY_USING, GENDER::STRING AS GENDER,
        TRY_TO_DATE(DOB, 'YYYY-MM-DD') AS DOB, TRY_TO_DATE(ANNIVERSARY, 'YYYY-MM-DD') AS ANNIVERSARY,
        PREFERENCES::STRING AS PREFERENCES,
        TRY_TO_TIMESTAMP_TZ(CREATEDDATE, 'YYYY-MM-DD"T"HH24:MI:SS.FF6') AS CREATED_DT,
        TRY_TO_TIMESTAMP_TZ(MODIFIEDDATE, 'YYYY-MM-DD"T"HH24:MI:SS.FF6') AS MODIFIED_DT,
        _STG_FILE_NAME, _STG_FILE_LOAD_TS, _STG_FILE_MD5, _COPY_DATA_TS
    FROM STAGE_SCH.CUSTOMER_STM
) AS source
ON target.CUSTOMER_ID = source.CUSTOMER_ID
WHEN MATCHED THEN
    UPDATE SET
        target.NAME = source.NAME, target.MOBILE = source.MOBILE,
        target.EMAIL = source.EMAIL, target.LOGIN_BY_USING = source.LOGIN_BY_USING,
        target.GENDER = source.GENDER, target.DOB = source.DOB,
        target.ANNIVERSARY = source.ANNIVERSARY, target.PREFERENCES = source.PREFERENCES,
        target.CREATED_DT = source.CREATED_DT, target.MODIFIED_DT = source.MODIFIED_DT,
        target._STG_FILE_NAME = source._STG_FILE_NAME, target._STG_FILE_LOAD_TS = source._STG_FILE_LOAD_TS,
        target._STG_FILE_MD5 = source._STG_FILE_MD5, target._COPY_DATA_TS = source._COPY_DATA_TS
WHEN NOT MATCHED THEN
    INSERT (
        CUSTOMER_ID, NAME, MOBILE, EMAIL, LOGIN_BY_USING, GENDER, DOB, ANNIVERSARY,
        PREFERENCES, CREATED_DT, MODIFIED_DT, _STG_FILE_NAME, _STG_FILE_LOAD_TS, _STG_FILE_MD5, _COPY_DATA_TS
    )
    VALUES (
        source.CUSTOMER_ID, source.NAME, source.MOBILE, source.EMAIL,
        source.LOGIN_BY_USING, source.GENDER, source.DOB, source.ANNIVERSARY,
        source.PREFERENCES, source.CREATED_DT, source.MODIFIED_DT,
        source._STG_FILE_NAME, source._STG_FILE_LOAD_TS, source._STG_FILE_MD5, source._COPY_DATA_TS
    );


-- CONSUMPTION LAYER (Customer Dim - SCD Type 2)
use schema consumption_sch;
CREATE OR REPLACE TABLE CONSUMPTION_SCH.CUSTOMER_DIM (
    CUSTOMER_HK NUMBER PRIMARY KEY,
    CUSTOMER_ID STRING NOT NULL,
    NAME STRING(100) NOT NULL,
    MOBILE STRING(15) WITH TAG (common.pii_policy_tag = 'PII'),
    EMAIL STRING(100) WITH TAG (common.pii_policy_tag = 'EMAIL'),
    LOGIN_BY_USING STRING(50),
    GENDER STRING(10) WITH TAG (common.pii_policy_tag = 'PII'),
    DOB DATE WITH TAG (common.pii_policy_tag = 'PII'),
    ANNIVERSARY DATE, PREFERENCES STRING,
    EFF_START_DATE TIMESTAMP_TZ, EFF_END_DATE TIMESTAMP_TZ, IS_CURRENT BOOLEAN
)
COMMENT = 'Customer Dimension table with SCD Type 2 handling for historical tracking.';


-- Merge from Clean Stream to Consumption Dim (Customer - SCD Type 2 Logic)
MERGE INTO
    CONSUMPTION_SCH.CUSTOMER_DIM AS target
USING
    CLEAN_SCH.CUSTOMER_STM AS source
ON
    target.CUSTOMER_ID = source.CUSTOMER_ID AND
    target.IS_CURRENT = TRUE -- Only check against the current record
WHEN MATCHED
    AND source.METADATA$ACTION = 'UPDATE' THEN
    -- Expire current record (SCD Type 2)
    UPDATE SET
        target.EFF_END_DATE = CURRENT_TIMESTAMP(),
        target.IS_CURRENT = FALSE
WHEN NOT MATCHED
    AND source.METADATA$ACTION = 'INSERT' THEN
    -- Insert new or updated record
    INSERT (
        CUSTOMER_HK, CUSTOMER_ID, NAME, MOBILE, EMAIL, LOGIN_BY_USING, GENDER, DOB,
        ANNIVERSARY, PREFERENCES, EFF_START_DATE, EFF_END_DATE, IS_CURRENT
    )
    VALUES (
        hash(SHA1_hex(CONCAT(source.CUSTOMER_ID, source.NAME, source.MOBILE,
            source.EMAIL, source.LOGIN_BY_USING, source.GENDER, source.DOB,
            source.ANNIVERSARY, source.PREFERENCES))),
        source.CUSTOMER_ID, source.NAME, source.MOBILE, source.EMAIL,
        source.LOGIN_BY_USING, source.GENDER, source.DOB, source.ANNIVERSARY,
        source.PREFERENCES, CURRENT_TIMESTAMP(), NULL, TRUE
    );


-- Delta Load (Customer) - Insert
use schema stage_sch;
copy into stage_sch.customer (customerid, name, mobile, email, loginbyusing, gender, dob, anniversary,
                          preferences, createddate, modifieddate,
                          _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as customerid, t.$2::text as name, t.$3::text as mobile,
        t.$4::text as email, t.$5::text as loginbyusing, t.$6::text as gender,
        t.$7::text as dob, t.$8::text as anniversary, t.$9::text as preferences,
        t.$10::text as createddate, t.$11::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/delta/customer/day-01-insert-customer.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;

-- Delta Load (Customer) - Insert/Update
copy into stage_sch.customer (customerid, name, mobile, email, loginbyusing, gender, dob, anniversary,
                          preferences, createddate, modifieddate,
                          _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as customerid, t.$2::text as name, t.$3::text as mobile,
        t.$4::text as email, t.$5::text as loginbyusing, t.$6::text as gender,
        t.$7::text as dob, t.$8::text as anniversary, t.$9::text as preferences,
        t.$10::text as createddate, t.$11::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/delta/customer/day-02-insert-update.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
-- ---------------------------------------------------------------------------------------------------------------------


-- 5. Customer Address Dimension Data Pipeline (SCD Type 2)
-- ---------------------------------------------------------------------------------------------------------------------

-- STAGE LAYER (Customer Address)
use schema stage_sch;
create or replace table stage_sch.customeraddress (
    addressid text, customerid text comment 'Customer FK (Source Data)', flatno text,
    houseno text, floor text, building text, landmark text, locality text,
    city text, state text, pincode text, coordinates text, primaryflag text,
    addresstype text, createddate text, modifieddate text,
    _stg_file_name text, _stg_file_load_ts timestamp, _stg_file_md5 text,
    _copy_data_ts timestamp default current_timestamp
)
comment = 'Customer address stage/raw table.';

create or replace stream stage_sch.customeraddress_stm
on table stage_sch.customeraddress
append_only = true
comment = 'Append-only stream on customer address table.';

-- Initial Load (Customer Address)
copy into stage_sch.customeraddress (addressid, customerid, flatno, houseno, floor, building,
                                 landmark, locality,city,pincode, state, coordinates, primaryflag, addresstype,
                                 createddate, modifieddate,
                                 _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as addressid, t.$2::text as customerid, t.$3::text as flatno,
        t.$4::text as houseno, t.$5::text as floor, t.$6::text as building,
        t.$7::text as landmark, t.$8::text as locality, t.$9::text as city,
        t.$10::text as State, t.$11::text as Pincode, t.$12::text as coordinates,
        t.$13::text as primaryflag, t.$14::text as addresstype, t.$15::text as createddate,
        t.$16::text as modifieddate, metadata$filename as _stg_file_name,
        metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/initial/customer-address t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;


-- CLEAN LAYER (Customer Address)
use schema clean_sch;
CREATE OR REPLACE TABLE CLEAN_SCH.CUSTOMER_ADDRESS (
    CUSTOMER_ADDRESS_SK NUMBER AUTOINCREMENT PRIMARY KEY comment 'Surrogate Key (EWH)',
    ADDRESS_ID INT comment 'Primary Key (Source Data)',
    CUSTOMER_ID_FK INT comment 'Customer FK (Source Data)', FLAT_NO STRING, HOUSE_NO STRING,
    FLOOR STRING, BUILDING STRING, LANDMARK STRING, locality STRING, CITY STRING,
    STATE STRING, PINCODE STRING, COORDINATES STRING, PRIMARY_FLAG STRING,
    ADDRESS_TYPE STRING, CREATED_DATE TIMESTAMP_TZ, MODIFIED_DATE TIMESTAMP_TZ,
    _STG_FILE_NAME STRING, _STG_FILE_LOAD_TS TIMESTAMP, _STG_FILE_MD5 STRING,
    _COPY_DATA_TS TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
comment = 'Customer address entity under clean schema with appropriate data type (SCD1).';

create or replace stream CLEAN_SCH.CUSTOMER_ADDRESS_STM
on table CLEAN_SCH.CUSTOMER_ADDRESS
comment = 'Standard stream on customer address entity to track changes.';


-- Merge from Stage Stream to Clean (Customer Address)
MERGE INTO clean_sch.customer_address AS clean
USING (
    SELECT
        CAST(addressid AS INT) AS address_id, CAST(customerid AS INT) AS customer_id_fk,
        flatno AS flat_no, houseno AS house_no, floor, building, landmark, locality,
        city, state, pincode, coordinates, primaryflag AS primary_flag,
        addresstype AS address_type,
        TRY_TO_TIMESTAMP_TZ(createddate, 'YYYY-MM-DD"T"HH24:MI:SS') AS created_date,
        TRY_TO_TIMESTAMP_TZ(modifieddate, 'YYYY-MM-DD"T"HH24:MI:SS') AS modified_date,
        _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts
    FROM stage_sch.customeraddress_stm
) AS stage
ON clean.address_id = stage.address_id
WHEN NOT MATCHED THEN
    INSERT (
        address_id, customer_id_fk, flat_no, house_no, floor, building, landmark,
        locality, city, state, pincode, coordinates, primary_flag, address_type,
        created_date, modified_date, _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts
    )
    VALUES (
        stage.address_id, stage.customer_id_fk, stage.flat_no, stage.house_no, stage.floor,
        stage.building, stage.landmark, stage.locality, stage.city, stage.state,
        stage.pincode, stage.coordinates, stage.primary_flag, stage.address_type,
        stage.created_date, stage.modified_date, stage._stg_file_name,
        stage._stg_file_load_ts, stage._stg_file_md5, stage._copy_data_ts
    )
WHEN MATCHED THEN
    UPDATE SET
        clean.flat_no = stage.flat_no, clean.house_no = stage.house_no, clean.floor = stage.floor,
        clean.building = stage.building, clean.landmark = stage.landmark, clean.locality = stage.locality,
        clean.city = stage.city, clean.state = stage.state, clean.pincode = stage.pincode,
        clean.coordinates = stage.coordinates, clean.primary_flag = stage.primary_flag,
        clean.address_type = stage.address_type, clean.created_date = stage.created_date,
        clean.modified_date = stage.modified_date, clean._stg_file_name = stage._stg_file_name,
        clean._stg_file_load_ts = stage._stg_file_load_ts, clean._stg_file_md5 = stage._stg_file_md5,
        clean._copy_data_ts = stage._copy_data_ts;


-- CONSUMPTION LAYER (Customer Address Dim - SCD Type 2)
use schema consumption_sch;
CREATE OR REPLACE TABLE CONSUMPTION_SCH.CUSTOMER_ADDRESS_DIM (
    CUSTOMER_ADDRESS_HK NUMBER PRIMARY KEY comment 'Customer Address HK (EDW)',
    ADDRESS_ID INT comment 'Primary Key (Source System)',
    CUSTOMER_ID_FK INT comment 'Customer FK (Source System)', -- Should align with business key type in Customer Dim
    FLAT_NO STRING, HOUSE_NO STRING, FLOOR STRING, BUILDING STRING, LANDMARK STRING,
    LOCALITY STRING, CITY STRING, STATE STRING, PINCODE STRING, COORDINATES STRING,
    PRIMARY_FLAG STRING, ADDRESS_TYPE STRING,
    EFF_START_DATE TIMESTAMP_TZ, EFF_END_DATE TIMESTAMP_TZ, IS_CURRENT BOOLEAN
);

-- Merge from Clean Stream to Consumption Dim (Customer Address - SCD Type 2 Logic)
MERGE INTO
    CONSUMPTION_SCH.CUSTOMER_ADDRESS_DIM AS target
USING
    CLEAN_SCH.CUSTOMER_ADDRESS_STM AS source
ON
    target.ADDRESS_ID = source.ADDRESS_ID AND
    target.IS_CURRENT = TRUE
WHEN MATCHED
    AND source.METADATA$ACTION = 'UPDATE' THEN
    -- Expire current record (SCD Type 2)
    UPDATE SET
        target.EFF_END_DATE = CURRENT_TIMESTAMP(),
        target.IS_CURRENT = FALSE
WHEN NOT MATCHED
    AND source.METADATA$ACTION = 'INSERT' THEN
    -- Insert new or updated record
    INSERT (
        CUSTOMER_ADDRESS_HK, ADDRESS_ID, CUSTOMER_ID_FK, FLAT_NO, HOUSE_NO, FLOOR,
        BUILDING, LANDMARK, LOCALITY, CITY, STATE, PINCODE, COORDINATES,
        PRIMARY_FLAG, ADDRESS_TYPE, EFF_START_DATE, EFF_END_DATE, IS_CURRENT
    )
    VALUES (
        hash(SHA1_hex(CONCAT(source.ADDRESS_ID, source.CUSTOMER_ID_FK, source.FLAT_NO,
            source.HOUSE_NO, source.FLOOR, source.BUILDING, source.LANDMARK,
            source.LOCALITY, source.CITY, source.STATE, source.PINCODE,
            source.COORDINATES, source.PRIMARY_FLAG, source.ADDRESS_TYPE))),
        source.ADDRESS_ID, source.CUSTOMER_ID_FK, source.FLAT_NO, source.HOUSE_NO,
        source.FLOOR, source.BUILDING, source.LANDMARK, source.LOCALITY,
        source.CITY, source.STATE, source.PINCODE, source.COORDINATES,
        source.PRIMARY_FLAG, source.ADDRESS_TYPE,
        CURRENT_TIMESTAMP(), NULL, TRUE
    );


-- Delta Load (Customer Address)
use schema stage_sch;
copy into stage_sch.customeraddress (addressid, customerid, flatno, houseno, floor, building,
                                 landmark, locality,city,pincode, state, coordinates, primaryflag, addresstype,
                                 createddate, modifieddate,
                                 _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as addressid, t.$2::text as customerid, t.$3::text as flatno,
        t.$4::text as houseno, t.$5::text as floor, t.$6::text as building,
        t.$7::text as landmark, t.$8::text as locality, t.$9::text as city,
        t.$10::text as State, t.$11::text as Pincode, t.$12::text as coordinates,
        t.$13::text as primaryflag, t.$14::text as addresstype, t.$15::text as createddate,
        t.$16::text as modifieddate, metadata$filename as _stg_file_name,
        metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/delta/customer-address t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
-- ---------------------------------------------------------------------------------------------------------------------


-- 6. Menu Dimension Data Pipeline (SCD Type 2)
-- ---------------------------------------------------------------------------------------------------------------------

-- STAGE LAYER (Menu)
use schema stage_sch;
create or replace table stage_sch.menu (
    menuid text comment 'Primary Key (Source System)', restaurantid text comment 'Restaurant FK(Source System)',
    itemname text, description text, price text, category text, availability text,
    itemtype text, createddate text, modifieddate text,
    _stg_file_name text, _stg_file_load_ts timestamp, _stg_file_md5 text,
    _copy_data_ts timestamp default current_timestamp
)
comment = 'Menu stage/raw table.';

create or replace stream stage_sch.menu_stm
on table stage_sch.menu
append_only = true
comment = 'Append-only stream on menu table.';

-- Initial Load (Menu)
copy into stage_sch.menu (menuid, restaurantid, itemname, description, price, category,
                         availability, itemtype, createddate, modifieddate,
                         _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as menuid, t.$2::text as restaurantid, t.$3::text as itemname,
        t.$4::text as description, t.$5::text as price, t.$6::text as category,
        t.$7::text as availability, t.$8::text as itemtype, t.$9::text as createddate,
        t.$10::text as modifieddate, metadata$filename as _stg_file_name,
        metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/initial/menu t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;


-- CLEAN LAYER (Menu)
use schema clean_sch;
CREATE OR REPLACE TABLE clean_sch.menu (
    Menu_SK INT AUTOINCREMENT PRIMARY KEY comment 'Surrogate Key (EDW)',
    Menu_ID INT NOT NULL UNIQUE comment 'Primary Key (Source System)',
    Restaurant_ID_FK INT comment 'Restaurant FK(Source System)' ,
    Item_Name STRING not null, Description STRING not null, Price DECIMAL(10, 2) not null,
    Category STRING, Availability BOOLEAN, Item_Type STRING,
    Created_dt TIMESTAMP_NTZ, Modified_dt TIMESTAMP_NTZ,
    _STG_FILE_NAME STRING, _STG_FILE_LOAD_TS TIMESTAMP_NTZ,
    _STG_FILE_MD5 STRING, _COPY_DATA_TS TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP
)
comment = 'Menu entity under clean schema with appropriate data type (SCD1).';

create or replace stream CLEAN_SCH.menu_stm
on table CLEAN_SCH.menu
comment = 'Standard stream on menu table to track changes.';


-- Merge from Stage Stream to Clean (Menu)
MERGE INTO clean_sch.menu AS target
USING (
    SELECT
        TRY_CAST(menuid AS INT) AS Menu_ID, TRY_CAST(restaurantid AS INT) AS Restaurant_ID_FK,
        TRIM(itemname) AS Item_Name, TRIM(description) AS Description,
        TRY_CAST(price AS DECIMAL(10, 2)) AS Price, TRIM(category) AS Category,
        CASE
            WHEN LOWER(availability) = 'true' THEN TRUE
            WHEN LOWER(availability) = 'false' THEN FALSE
            ELSE NULL
        END AS Availability,
        TRIM(itemtype) AS Item_Type,
        TRY_CAST(createddate AS TIMESTAMP_NTZ) AS Created_dt,
        TRY_CAST(modifieddate AS TIMESTAMP_NTZ) AS Modified_dt,
        _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts
    FROM stage_sch.menu_stm
) AS source
ON target.Menu_ID = source.Menu_ID
WHEN MATCHED THEN
    UPDATE SET
        Restaurant_ID_FK = source.Restaurant_ID_FK, Item_Name = source.Item_Name,
        Description = source.Description, Price = source.Price, Category = source.Category,
        Availability = source.Availability, Item_Type = source.Item_Type,
        Created_dt = source.Created_dt, Modified_dt = source.Modified_dt,
        _STG_FILE_NAME = source._stg_file_name, _STG_FILE_LOAD_TS = source._stg_file_load_ts,
        _STG_FILE_MD5 = source._stg_file_md5, _COPY_DATA_TS = CURRENT_TIMESTAMP
WHEN NOT MATCHED THEN
    INSERT (
        Menu_ID, Restaurant_ID_FK, Item_Name, Description, Price, Category,
        Availability, Item_Type, Created_dt, Modified_dt,
        _STG_FILE_NAME, _STG_FILE_LOAD_TS, _STG_FILE_MD5, _COPY_DATA_TS
    )
    VALUES (
        source.Menu_ID, source.Restaurant_ID_FK, source.Item_Name, source.Description,
        source.Price, source.Category, source.Availability, source.Item_Type,
        source.Created_dt, source.Modified_dt,
        source._stg_file_name, source._stg_file_load_ts, source._stg_file_md5, CURRENT_TIMESTAMP
    );


-- CONSUMPTION LAYER (Menu Dim - SCD Type 2)
use schema consumption_sch;
CREATE OR REPLACE TABLE consumption_sch.menu_dim (
    Menu_Dim_HK NUMBER primary key comment 'Menu Dim HK (EDW)',
    Menu_ID INT NOT NULL comment 'Primary Key (Source System)',
    Restaurant_ID_FK INT NOT NULL comment 'Restaurant FK (Source System)',
    Item_Name STRING, Description STRING, Price DECIMAL(10, 2), Category STRING,
    Availability BOOLEAN, Item_Type STRING,
    EFF_START_DATE TIMESTAMP_NTZ, EFF_END_DATE TIMESTAMP_NTZ, IS_CURRENT BOOLEAN
)
COMMENT = 'Menu Dimension table with SCD Type 2.';


-- Merge from Clean Stream to Consumption Dim (Menu - SCD Type 2 Logic)
MERGE INTO
    consumption_sch.MENU_DIM AS target
USING
    CLEAN_SCH.MENU_STM AS source
ON
    target.Menu_ID = source.Menu_ID AND
    target.IS_CURRENT = TRUE
WHEN MATCHED
    AND source.METADATA$ACTION = 'UPDATE' THEN
    -- Expire current record (SCD Type 2)
    UPDATE SET
        target.EFF_END_DATE = CURRENT_TIMESTAMP(),
        target.IS_CURRENT = FALSE
WHEN NOT MATCHED
    AND source.METADATA$ACTION = 'INSERT' THEN
    -- Insert new or updated record
    INSERT (
        Menu_Dim_HK, Menu_ID, Restaurant_ID_FK, Item_Name, Description, Price,
        Category, Availability, Item_Type, EFF_START_DATE, EFF_END_DATE, IS_CURRENT
    )
    VALUES (
        hash(SHA1_hex(CONCAT(source.Menu_ID, source.Restaurant_ID_FK,
            source.Item_Name, source.Description, source.Price,
            source.Category, source.Availability, source.Item_Type))),
        source.Menu_ID, source.Restaurant_ID_FK, source.Item_Name,
        source.Description, source.Price, source.Category,
        source.Availability, source.Item_Type,
        CURRENT_TIMESTAMP(), NULL, TRUE
    );


-- Delta Load (Menu)
use schema stage_sch;
copy into stage_sch.menu (menuid, restaurantid, itemname, description, price, category,
                         availability, itemtype, createddate, modifieddate,
                         _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as menuid, t.$2::text as restaurantid, t.$3::text as itemname,
        t.$4::text as description, t.$5::text as price, t.$6::text as category,
        t.$7::text as availability, t.$8::text as itemtype, t.$9::text as createddate,
        t.$10::text as modifieddate, metadata$filename as _stg_file_name,
        metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/delta/menu t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
-- ---------------------------------------------------------------------------------------------------------------------


-- 7. Delivery Agent Dimension Data Pipeline (SCD Type 2)
-- ---------------------------------------------------------------------------------------------------------------------

-- STAGE LAYER (Delivery Agent)
use schema stage_sch;
create or replace table stage_sch.deliveryagent (
    deliveryagentid text comment 'Primary Key (Source System)', name text, phone text,
    vehicletype text, locationid text, status text, gender text, rating text,
    createddate text, modifieddate text,
    _stg_file_name text, _stg_file_load_ts timestamp, _stg_file_md5 text,
    _copy_data_ts timestamp default current_timestamp
)
comment = 'Delivery agent stage/raw table.';

create or replace stream stage_sch.deliveryagent_stm
on table stage_sch.deliveryagent
append_only = true
comment = 'Append-only stream on delivery agent table.';

-- Initial Load (Delivery Agent)
copy into stage_sch.deliveryagent (deliveryagentid, name, phone, vehicletype, locationid,
                                 status, gender, rating, createddate, modifieddate,
                                 _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as deliveryagentid, t.$2::text as name, t.$3::text as phone,
        t.$4::text as vehicletype, t.$5::text as locationid, t.$6::text as status,
        t.$7::text as gender, t.$8::text as rating, t.$9::text as createddate,
        t.$10::text as modifieddate, metadata$filename as _stg_file_name,
        metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/initial/delivery-agent t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;


-- CLEAN LAYER (Delivery Agent)
use schema clean_sch;
CREATE OR REPLACE TABLE clean_sch.delivery_agent (
    delivery_agent_sk INT AUTOINCREMENT PRIMARY KEY comment 'Surrogate Key (EDW)',
    delivery_agent_id INT NOT NULL UNIQUE comment 'Primary Key (Source System)',
    name STRING NOT NULL, phone STRING NOT NULL, vehicle_type STRING NOT NULL,
    location_id_fk INT comment 'Location FK(Source System)', status STRING, gender STRING,
    rating number(4,2), created_dt TIMESTAMP_NTZ, modified_dt TIMESTAMP_NTZ,
    _stg_file_name STRING, _stg_file_load_ts TIMESTAMP, _stg_file_md5 STRING,
    _copy_data_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
comment = 'Delivery agent entity under clean schema with appropriate data type (SCD1).';

create or replace stream CLEAN_SCH.delivery_agent_stm
on table CLEAN_SCH.delivery_agent
comment = 'Standard stream on clean delivery agent table to track changes.';


-- Merge from Stage Stream to Clean (Delivery Agent)
MERGE INTO clean_sch.delivery_agent AS target
USING stage_sch.deliveryagent_stm AS source
ON target.delivery_agent_id = TRY_TO_NUMBER(source.deliveryagentid)
WHEN MATCHED THEN
    UPDATE SET
        target.phone = source.phone, target.vehicle_type = source.vehicletype,
        target.location_id_fk = TRY_TO_NUMBER(source.locationid), target.status = source.status,
        target.gender = source.gender, target.rating = TRY_TO_DECIMAL(source.rating,4,2),
        target.created_dt = TRY_TO_TIMESTAMP(source.createddate),
        target.modified_dt = TRY_TO_TIMESTAMP(source.modifieddate),
        target._stg_file_name = source._stg_file_name,
        target._stg_file_load_ts = source._stg_file_load_ts,
        target._stg_file_md5 = source._stg_file_md5,
        target._copy_data_ts = source._copy_data_ts
WHEN NOT MATCHED THEN
    INSERT (
        delivery_agent_id, name, phone, vehicle_type, location_id_fk, status,
        gender, rating, created_dt, modified_dt, _stg_file_name,
        _stg_file_load_ts, _stg_file_md5, _copy_data_ts
    )
    VALUES (
        TRY_TO_NUMBER(source.deliveryagentid), source.name, source.phone,
        source.vehicletype, TRY_TO_NUMBER(source.locationid), source.status,
        source.gender, TRY_TO_NUMBER(source.rating), TRY_TO_TIMESTAMP(source.createddate),
        TRY_TO_TIMESTAMP(source.modifieddate), source._stg_file_name,
        source._stg_file_load_ts, source._stg_file_md5, CURRENT_TIMESTAMP()
    );


-- CONSUMPTION LAYER (Delivery Agent Dim - SCD Type 2)
use schema consumption_sch;
CREATE OR REPLACE TABLE consumption_sch.delivery_agent_dim (
    delivery_agent_hk number primary key comment 'Delivery Agend Dim HK (EDW)',
    delivery_agent_id NUMBER not null comment 'Primary Key (Source System)',
    name STRING NOT NULL, phone STRING UNIQUE, vehicle_type STRING,
    location_id_fk NUMBER NOT NULL comment 'Location FK (Source System)',
    status STRING, gender STRING, rating NUMBER(4,2),
    eff_start_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP, eff_end_date TIMESTAMP,
    is_current BOOLEAN DEFAULT TRUE
)
comment = 'Dim table for delivery agent entity with SCD2 support.';


-- Merge from Clean Stream to Consumption Dim (Delivery Agent - SCD Type 2 Logic)
MERGE INTO consumption_sch.delivery_agent_dim AS target
USING CLEAN_SCH.delivery_agent_stm AS source
ON
    target.delivery_agent_id = source.delivery_agent_id AND
    target.is_current = TRUE
WHEN MATCHED
    AND source.METADATA$ACTION = 'UPDATE' THEN
    -- Expire current record (SCD Type 2)
    UPDATE SET
        target.eff_end_date = CURRENT_TIMESTAMP,
        target.is_current = FALSE
WHEN NOT MATCHED
    AND source.METADATA$ACTION = 'INSERT' THEN
    -- Insert new or updated record
    INSERT (
        delivery_agent_hk, delivery_agent_id, name, phone, vehicle_type,
        location_id_fk, status, gender, rating, eff_start_date, eff_end_date, is_current
    )
    VALUES (
        hash(SHA1_HEX(CONCAT(source.delivery_agent_id, source.name, source.phone,
            source.vehicle_type, source.location_id_fk, source.status,
            source.gender, source.rating))),
        source.delivery_agent_id, source.name, source.phone,
        source.vehicle_type, source.location_id_fk, source.status,
        source.gender, source.rating,
        CURRENT_TIMESTAMP, NULL, TRUE
    );


-- Delta Load (Delivery Agent)
use schema stage_sch;
copy into deliveryagent (deliveryagentid, name, phone, vehicletype, locationid,
                          status, gender, rating, createddate, modifieddate,
                          _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as deliveryagentid, t.$2::text as name, t.$3::text as phone,
        t.$4::text as vehicletype, t.$5::text as locationid, t.$6::text as status,
        t.$7::text as gender, t.$8::text as rating, t.$9::text as createddate,
        t.$10::text as modifieddate, metadata$filename as _stg_file_name,
        metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/delta/delivery-agent/day-02-delivery-agent.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
-- ---------------------------------------------------------------------------------------------------------------------


-- 8. Delivery Entity Pipeline (No SCD)
-- ---------------------------------------------------------------------------------------------------------------------

-- STAGE LAYER (Delivery)
use schema stage_sch;
create or replace table stage_sch.delivery (
    deliveryid text comment 'Primary Key (Source System)', orderid text comment 'Order FK (Source System)',
    deliveryagentid text comment 'Delivery Agent FK(Source System)', deliverystatus text,
    estimatedtime text, addressid text comment 'Customer Address FK(Source System)',
    deliverydate text, createddate text, modifieddate text,
    _stg_file_name text, _stg_file_load_ts timestamp, _stg_file_md5 text,
    _copy_data_ts timestamp default current_timestamp
)
comment = 'Delivery stage/raw table.';

create or replace stream stage_sch.delivery_stm
on table stage_sch.delivery
append_only = true
comment = 'Append-only stream on delivery table.';

-- Initial Load (Delivery)
copy into stage_sch.delivery (deliveryid,orderid, deliveryagentid, deliverystatus,
                              estimatedtime, addressid, deliverydate, createddate,
                              modifieddate, _stg_file_name, _stg_file_load_ts,
                              _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as deliveryid, t.$2::text as orderid, t.$3::text as deliveryagentid,
        t.$4::text as deliverystatus, t.$5::text as estimatedtime, t.$6::text as addressid,
        t.$7::text as deliverydate, t.$8::text as createddate, t.$9::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/initial/delivery/delivery-initial-load.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;


-- CLEAN LAYER (Delivery)
use schema clean_sch;
CREATE OR REPLACE TABLE clean_sch.delivery (
    delivery_sk INT AUTOINCREMENT PRIMARY KEY comment 'Surrogate Key (EDW)',
    delivery_id INT NOT NULL comment 'Primary Key (Source System)',
    order_id_fk NUMBER NOT NULL comment 'Order FK (Source System)',
    delivery_agent_id_fk NUMBER NOT NULL comment 'Delivery Agent FK (Source System)',
    delivery_status STRING, estimated_time STRING,
    customer_address_id_fk NUMBER NOT NULL comment 'Customer Address FK (Source System)',
    delivery_date TIMESTAMP, created_date TIMESTAMP, modified_date TIMESTAMP,
    _stg_file_name STRING, _stg_file_load_ts TIMESTAMP, _stg_file_md5 STRING,
    _copy_data_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
comment = 'Delivery entity under clean schema with appropriate data type (No SCD).';

create or replace stream CLEAN_SCH.delivery_stm
on table CLEAN_SCH.delivery
comment = 'Standard stream on delivery table to track changes.';

-- Merge from Stage Stream to Clean (Delivery)
MERGE INTO
    clean_sch.delivery AS target
USING
    stage_sch.delivery_stm AS source
ON
    target.delivery_id = TO_NUMBER(source.deliveryid)
WHEN MATCHED THEN
    UPDATE SET
        delivery_status = source.deliverystatus,
        estimated_time = source.estimatedtime,
        customer_address_id_fk = TO_NUMBER(source.addressid),
        delivery_date = TO_TIMESTAMP(source.deliverydate),
        created_date = TO_TIMESTAMP(source.createddate),
        modified_date = TO_TIMESTAMP(source.modifieddate),
        _stg_file_name = source._stg_file_name,
        _stg_file_load_ts = source._stg_file_load_ts,
        _stg_file_md5 = source._stg_file_md5,
        _copy_data_ts = source._copy_data_ts
WHEN NOT MATCHED THEN
    INSERT (
        delivery_id, order_id_fk, delivery_agent_id_fk, delivery_status,
        estimated_time, customer_address_id_fk, delivery_date, created_date,
        modified_date, _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts
    )
    VALUES (
        TO_NUMBER(source.deliveryid), TO_NUMBER(source.orderid), TO_NUMBER(source.deliveryagentid),
        source.deliverystatus, source.estimatedtime, TO_NUMBER(source.addressid),
        TO_TIMESTAMP(source.deliverydate), TO_TIMESTAMP(source.createddate),
        TO_TIMESTAMP(source.modifieddate), source._stg_file_name,
        source._stg_file_load_ts, source._stg_file_md5, source._copy_data_ts
    );
-- ---------------------------------------------------------------------------------------------------------------------


-- 9. Order Entity Pipeline (No SCD)
-- ---------------------------------------------------------------------------------------------------------------------

-- STAGE LAYER (Orders)
use schema stage_sch;
create or replace table stage_sch.orders (
    orderid text comment 'Primary Key (Source System)', customerid text comment 'Customer FK(Source System)',
    restaurantid text comment 'Restaurant FK(Source System)', orderdate text,
    totalamount text, status text, paymentmethod text, createddate text, modifieddate text,
    _stg_file_name text, _stg_file_load_ts timestamp, _stg_file_md5 text,
    _copy_data_ts timestamp default current_timestamp
)
comment = 'Order stage/raw table.';

create or replace stream stage_sch.orders_stm
on table stage_sch.orders
append_only = true
comment = 'Append-only stream on orders table.';

-- Initial Load (Orders)
copy into stage_sch.orders (orderid, customerid, restaurantid, orderdate, totalamount,
                            status, paymentmethod, createddate, modifieddate,
                            _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as orderid, t.$2::text as customerid, t.$3::text as restaurantid,
        t.$4::text as orderdate, t.$5::text as totalamount, t.$6::text as status,
        t.$7::text as paymentmethod, t.$8::text as createddate, t.$9::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/initial/order/orders-initial.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
list@stage_sch.csv_stg/initial;

-- CLEAN LAYER (Orders)
use schema clean_sch;
CREATE OR REPLACE TABLE CLEAN_SCH.ORDERS (
    ORDER_SK NUMBER AUTOINCREMENT PRIMARY KEY comment 'Surrogate Key (EDW)',
    ORDER_ID BIGINT UNIQUE comment 'Primary Key (Source System)',
    CUSTOMER_ID_FK BIGINT comment 'Customer FK(Source System)',
    RESTAURANT_ID_FK BIGINT comment 'Restaurant FK(Source System)',
    ORDER_DATE TIMESTAMP, TOTAL_AMOUNT DECIMAL(10, 2), STATUS STRING,
    PAYMENT_METHOD STRING, created_dt timestamp_tz, modified_dt timestamp_tz,
    _stg_file_name string, _stg_file_load_ts timestamp_ntz,
    _stg_file_md5 string, _copy_data_ts timestamp_ntz default current_timestamp
)
comment = 'Order entity under clean schema with appropriate data type (No SCD).';

create or replace stream CLEAN_SCH.ORDERS_stm
on table CLEAN_SCH.ORDERS
comment = 'Standard stream on orders table to track changes.';


-- Merge from Stage Stream to Clean (Orders)
MERGE INTO CLEAN_SCH.ORDERS AS target
USING STAGE_SCH.ORDERS_STM AS source
    ON target.ORDER_ID = TRY_TO_NUMBER(source.ORDERID)
WHEN MATCHED THEN
    UPDATE SET
        TOTAL_AMOUNT = TRY_TO_DECIMAL(source.TOTALAMOUNT),
        STATUS = source.STATUS,
        PAYMENT_METHOD = source.PAYMENTMETHOD,
        MODIFIED_DT = TRY_TO_TIMESTAMP_TZ(source.MODIFIEDDATE),
        _STG_FILE_NAME = source._STG_FILE_NAME,
        _STG_FILE_LOAD_TS = source._STG_FILE_LOAD_TS,
        _STG_FILE_MD5 = source._STG_FILE_MD5,
        _COPY_DATA_TS = CURRENT_TIMESTAMP
WHEN NOT MATCHED THEN
    INSERT (
        ORDER_ID, CUSTOMER_ID_FK, RESTAURANT_ID_FK, ORDER_DATE, TOTAL_AMOUNT,
        STATUS, PAYMENT_METHOD, CREATED_DT, MODIFIED_DT,
        _STG_FILE_NAME, _STG_FILE_LOAD_TS, _STG_FILE_MD5, _COPY_DATA_TS
    )
    VALUES (
        TRY_TO_NUMBER(source.ORDERID), TRY_TO_NUMBER(source.CUSTOMERID),
        TRY_TO_NUMBER(source.RESTAURANTID), TRY_TO_TIMESTAMP(source.ORDERDATE),
        TRY_TO_DECIMAL(source.TOTALAMOUNT), source.STATUS, source.PAYMENTMETHOD,
        TRY_TO_TIMESTAMP_TZ(source.CREATEDDATE), TRY_TO_TIMESTAMP_TZ(source.MODIFIEDDATE),
        source._STG_FILE_NAME, source._STG_FILE_LOAD_TS, source._STG_FILE_MD5, CURRENT_TIMESTAMP
    );


-- Delta Load (Orders)
use schema stage_sch;
copy into stage_sch.orders (orderid, customerid, restaurantid, orderdate, totalamount,
                            status, paymentmethod, createddate, modifieddate,
                            _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as orderid, t.$2::text as customerid, t.$3::text as restaurantid,
        t.$4::text as orderdate, t.$5::text as totalamount, t.$6::text as status,
        t.$7::text as paymentmethod, t.$8::text as createddate, t.$9::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/delta/order/day-02-orders.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
-- ---------------------------------------------------------------------------------------------------------------------


-- 10. Order Item Entity Pipeline (No SCD)
-- ---------------------------------------------------------------------------------------------------------------------

-- STAGE LAYER (Order Item)
use schema stage_sch;
create or replace table stage_sch.orderitem (
    orderitemid text comment 'Primary Key (Source System)', orderid text comment 'Order FK(Source System)',
    menuid text comment 'Menu FK(Source System)', quantity text, price text,
    subtotal text, createddate text, modifieddate text,
    _stg_file_name text, _stg_file_load_ts timestamp, _stg_file_md5 text,
    _copy_data_ts timestamp default current_timestamp
)
comment = 'Order item stage/raw table.';

create or replace stream stage_sch.orderitem_stm
on table stage_sch.orderitem
append_only = true
comment = 'Append-only stream on order item table.';

-- Initial Load (Order Item)
copy into stage_sch.orderitem (orderitemid, orderid, menuid, quantity, price,
                               subtotal, createddate, modifieddate,
                               _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as orderitemid, t.$2::text as orderid, t.$3::text as menuid,
        t.$4::text as quantity, t.$5::text as price, t.$6::text as subtotal,
        t.$7::text as createddate, t.$8::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @stage_sch.csv_stg/initial/order-item/order-Item-initial.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
list@stage_sch.csv_stg/initial;


-- CLEAN LAYER (Order Item)
use schema clean_sch;
CREATE OR REPLACE TABLE clean_sch.order_item (
    order_item_sk NUMBER AUTOINCREMENT primary key comment 'Surrogate Key (EDW)',
    order_item_id NUMBER NOT NULL UNIQUE comment 'Primary Key (Source System)',
    order_id_fk NUMBER NOT NULL comment 'Order FK(Source System)',
    menu_id_fk NUMBER NOT NULL comment 'Menu FK(Source System)',
    quantity NUMBER(10, 2), price NUMBER(10, 2), subtotal NUMBER(10, 2),
    created_dt TIMESTAMP, modified_dt TIMESTAMP,
    _stg_file_name VARCHAR(255), _stg_file_load_ts TIMESTAMP,
    _stg_file_md5 VARCHAR(255), _copy_data_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
comment = 'Order item entity under clean schema with appropriate data type (No SCD).';

create or replace stream CLEAN_SCH.order_item_stm
on table CLEAN_SCH.order_item
comment = 'Standard stream on order_item table to track changes.';


-- Merge from Stage Stream to Clean (Order Item)
MERGE INTO clean_sch.order_item AS target
USING (
    SELECT
        TRY_TO_NUMBER(orderitemid) AS order_item_id, TRY_TO_NUMBER(orderid) AS order_id_fk,
        TRY_TO_NUMBER(menuid) AS menu_id_fk, TRY_TO_NUMBER(quantity) AS quantity,
        TRY_TO_NUMBER(price) AS price, TRY_TO_NUMBER(subtotal) AS subtotal,
        TRY_TO_TIMESTAMP(createddate) AS created_dt, TRY_TO_TIMESTAMP(modifieddate) AS modified_dt,
        _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts
    FROM stage_sch.orderitem_stm
) AS source
ON
    target.order_item_id = source.order_item_id
WHEN MATCHED THEN
    UPDATE SET
        target.quantity = source.quantity, target.price = source.price,
        target.subtotal = source.subtotal, target.modified_dt = source.modified_dt,
        target._stg_file_name = source._stg_file_name,
        target._stg_file_load_ts = source._stg_file_load_ts,
        target._stg_file_md5 = source._stg_file_md5,
        target._copy_data_ts = source._copy_data_ts
WHEN NOT MATCHED THEN
    INSERT (
        order_item_id, order_id_fk, menu_id_fk, quantity, price, subtotal,
        created_dt, modified_dt, _stg_file_name, _stg_file_load_ts,
        _stg_file_md5, _copy_data_ts
    )
    VALUES (
        source.order_item_id, source.order_id_fk, source.menu_id_fk, source.quantity,
        source.price, source.subtotal, source.created_dt, source.modified_dt,
        source._stg_file_name, source._stg_file_load_ts,
        source._stg_file_md5, CURRENT_TIMESTAMP()
    );


-- Delta Load (Order Item)
use schema stage_sch;
copy into stage_sch.orderitem (orderitemid, orderid, menuid, quantity, price,
                               subtotal, createddate, modifieddate,
                               _stg_file_name, _stg_file_load_ts, _stg_file_md5, _copy_data_ts)
from (
    select
        t.$1::text as orderitemid, t.$2::text as orderid, t.$3::text as menuid,
        t.$4::text as quantity, t.$5::text as price, t.$6::text as subtotal,
        t.$7::text as createddate, t.$8::text as modifieddate,
        metadata$filename as _stg_file_name, metadata$file_last_modified as _stg_file_load_ts,
        metadata$file_content_key as _stg_file_md5, current_timestamp as _copy_data_ts
    from @csv_stg/delta/order-item/day-01-order-item.csv t
)
file_format = (format_name = 'stage_sch.csv_file_format')
on_error = abort_statement;
list@stage_sch.csv_stg/delta;
-- ---------------------------------------------------------------------------------------------------------------------


-- 11. Date Dimension Pipeline
-- ---------------------------------------------------------------------------------------------------------------------

use schema consumption_sch;
CREATE OR REPLACE TABLE CONSUMPTION_SCH.DATE_DIM (
    DATE_DIM_HK NUMBER PRIMARY KEY comment 'Menu Dim HK (EDW)',
    CALENDAR_DATE DATE UNIQUE, YEAR NUMBER, QUARTER NUMBER, MONTH NUMBER,
    WEEK NUMBER, DAY_OF_YEAR NUMBER, DAY_OF_WEEK NUMBER, DAY_OF_THE_MONTH NUMBER,
    DAY_NAME STRING
)
comment = 'Date dimension table created using min of order data.';

-- Insert into Date Dimension using Recursive CTE
INSERT INTO CONSUMPTION_SCH.DATE_DIM
WITH RECURSIVE my_date_dim_cte AS (
    -- Anchor clause
    SELECT
        DATE(min(order_date)) AS today,
        YEAR(today) AS year, QUARTER(today) AS quarter, MONTH(today) AS month,
        WEEK(today) AS week, DAYOFYEAR(today) AS day_of_year,
        DAYOFWEEK(today) AS day_of_week, DAY(today) AS day_of_the_month,
        DAYNAME(today) AS day_name
    FROM clean_sch.orders -- Find the earliest date

    UNION ALL

    -- Recursive clause
    SELECT
        DATEADD('day', 1, today) AS today_r, -- Use DATEADD instead of DATEADD('day', -1, today) to move forward
        YEAR(today_r) AS year, QUARTER(today_r) AS quarter, MONTH(today_r) AS month,
        WEEK(today_r) AS week, DAYOFYEAR(today_r) AS day_of_year,
        DAYOFWEEK(today_r) AS day_of_week, DAY(today_r) AS day_of_the_month,
        DAYNAME(today_r) AS day_name
    FROM
        my_date_dim_cte
    WHERE
        today_r <= CURRENT_DATE() -- Stop at today or a future date
)
SELECT
    hash(SHA1_hex(today)) AS DATE_DIM_HK, today AS CALENDAR_DATE, YEAR, QUARTER,
    MONTH, WEEK, DAY_OF_YEAR, DAY_OF_WEEK, DAY_OF_THE_MONTH, DAY_NAME
FROM my_date_dim_cte;
-- ---------------------------------------------------------------------------------------------------------------------


-- 12. Order Item Fact Table Pipeline
-- ---------------------------------------------------------------------------------------------------------------------

use schema consumption_sch;
CREATE OR REPLACE TABLE consumption_sch.order_item_fact (
    order_item_fact_sk NUMBER AUTOINCREMENT comment 'Surrogate Key (EDW)',
    order_item_id NUMBER comment 'Order Item FK (Source System)',
    order_id NUMBER comment 'Order FK (Source System)',
    customer_dim_key NUMBER comment 'Customer Dim HK',
    customer_address_dim_key NUMBER,
    restaurant_dim_key NUMBER,
    restaurant_location_dim_key NUMBER,
    menu_dim_key NUMBER,
    delivery_agent_dim_key NUMBER,
    order_date_dim_key NUMBER,
    quantity NUMBER,
    price NUMBER(10, 2),
    subtotal NUMBER(10, 2),
    delivery_status VARCHAR,
    estimated_time VARCHAR,
    -- Natural keys are kept for easy debugging, but the joins should use the surrogate keys/hash keys
    PRIMARY KEY (order_item_id, order_id) -- Composite key for uniqueness
)
comment = 'The item order fact table with item level metrics and dimensional keys.';


-- Merge from Clean Streams to Fact Table (Snapshot Fact)
MERGE INTO consumption_sch.order_item_fact AS target
USING (
    SELECT
        oi.Order_Item_ID AS order_item_id, oi.Order_ID_fk AS order_id,
        -- Dimension Key Lookups
        c.CUSTOMER_HK AS customer_dim_key,
        ca.CUSTOMER_ADDRESS_HK AS customer_address_dim_key,
        r.RESTAURANT_HK AS restaurant_dim_key,
        rl.restaurant_location_hk AS restaurant_location_dim_key,
        m.Menu_Dim_HK AS menu_dim_key,
        da.DELIVERY_AGENT_HK AS delivery_agent_dim_key,
        dd.DATE_DIM_HK AS order_date_dim_key,
        -- Measures and Degenerate Dimensions
        oi.Quantity AS quantity, oi.Price AS price, oi.Subtotal AS subtotal,
        d.delivery_status AS delivery_status, d.estimated_time AS estimated_time
    FROM
        clean_sch.order_item_stm oi -- Delta data on Order Item
    JOIN
        clean_sch.orders_stm o ON oi.Order_ID_fk = o.ORDER_ID -- Delta data on Order
    JOIN
        clean_sch.delivery_stm d ON o.ORDER_ID = d.Order_ID_fk -- Delta data on Delivery
    -- Look up surrogate keys from the latest dimensional records (IS_CURRENT = TRUE)
    JOIN
        consumption_sch.CUSTOMER_DIM c on o.Customer_ID_fk = c.CUSTOMER_ID AND c.IS_CURRENT = TRUE
    JOIN
        consumption_sch.CUSTOMER_ADDRESS_DIM ca on d.customer_address_id_fk = ca.ADDRESS_ID AND ca.IS_CURRENT = TRUE
    JOIN
        consumption_sch.restaurant_dim r on o.Restaurant_ID_fk = r.RESTAURANT_ID AND r.IS_CURRENT = TRUE
    JOIN
        consumption_sch.restaurant_location_dim rl on r.LOCATION_ID_FK = rl.LOCATION_ID -- SCD1: always current
    JOIN
        consumption_sch.menu_dim m ON oi.MENU_ID_fk = m.Menu_ID AND m.IS_CURRENT = TRUE
    JOIN
        consumption_sch.delivery_agent_dim da ON d.Delivery_Agent_ID_fk = da.delivery_agent_id AND da.IS_CURRENT = TRUE
    JOIN
        CONSUMPTION_SCH.DATE_DIM dd on dd.CALENDAR_DATE = DATE(o.ORDER_DATE)
) AS source_stm
ON
    target.order_item_id = source_stm.order_item_id AND
    target.order_id = source_stm.order_id
WHEN MATCHED THEN
    -- Update existing fact record (mainly for late-arriving delivery updates)
    UPDATE SET
        target.customer_dim_key = source_stm.customer_dim_key,
        target.customer_address_dim_key = source_stm.customer_address_dim_key,
        target.restaurant_dim_key = source_stm.restaurant_dim_key,
        target.restaurant_location_dim_key = source_stm.restaurant_location_dim_key,
        target.menu_dim_key = source_stm.menu_dim_key,
        target.delivery_agent_dim_key = source_stm.delivery_agent_dim_key,
        target.order_date_dim_key = source_stm.order_date_dim_key,
        target.quantity = source_stm.quantity,
        target.price = source_stm.price,
        target.subtotal = source_stm.subtotal,
        target.delivery_status = source_stm.delivery_status,
        target.estimated_time = source_stm.estimated_time
WHEN NOT MATCHED THEN
    -- Insert new fact record
    INSERT (
        order_item_id, order_id, customer_dim_key, customer_address_dim_key,
        restaurant_dim_key, restaurant_location_dim_key, menu_dim_key,
        delivery_agent_dim_key, order_date_dim_key, quantity, price, subtotal,
        delivery_status, estimated_time
    )
    VALUES (
        source_stm.order_item_id, source_stm.order_id, source_stm.customer_dim_key,
        source_stm.customer_address_dim_key, source_stm.restaurant_dim_key,
        source_stm.restaurant_location_dim_key, source_stm.menu_dim_key,
        source_stm.delivery_agent_dim_key, source_stm.order_date_dim_key,
        source_stm.quantity, source_stm.price, source_stm.subtotal,
        source_stm.delivery_status, source_stm.estimated_time
    );

-- Add Foreign Key Constraints (Best practice for metadata/query optimization, although not strictly enforced in Snowflake)
-- Add Foreign Key Constraints (These should now work)
ALTER TABLE consumption_sch.order_item_fact ADD CONSTRAINT fk_order_item_fact_customer_dim
    FOREIGN KEY (customer_dim_key) REFERENCES consumption_sch.customer_dim (customer_hk);

ALTER TABLE consumption_sch.order_item_fact ADD CONSTRAINT fk_order_item_fact_customer_address_dim
    FOREIGN KEY (customer_address_dim_key) REFERENCES consumption_sch.customer_address_dim (CUSTOMER_ADDRESS_HK);

ALTER TABLE consumption_sch.order_item_fact ADD CONSTRAINT fk_order_item_fact_restaurant_dim
    FOREIGN KEY (restaurant_dim_key) REFERENCES consumption_sch.restaurant_dim (restaurant_hk);

ALTER TABLE consumption_sch.order_item_fact ADD CONSTRAINT fk_order_item_fact_restaurant_location_dim
    FOREIGN KEY (restaurant_location_dim_key) REFERENCES consumption_sch.restaurant_location_dim (restaurant_location_hk);
    -- STEP 1: Define the Primary Key on the dimension table.
ALTER TABLE consumption_sch.restaurant_location_dim
ADD CONSTRAINT pk_restaurant_location_dim PRIMARY KEY (restaurant_location_hk);
-- STEP 2: Now the Foreign Key constraint will execute successfully.
ALTER TABLE consumption_sch.order_item_fact
ADD CONSTRAINT fk_order_item_fact_restaurant_location_dim
FOREIGN KEY (restaurant_location_dim_key) REFERENCES consumption_sch.restaurant_location_dim (restaurant_location_hk);
-- Syntax to add the Primary Key to the Parent Table
ALTER TABLE consumption_sch.restaurant_location_dim
ADD CONSTRAINT pk_restaurant_location PRIMARY KEY (restaurant_location_hk);

ALTER TABLE consumption_sch.order_item_fact ADD CONSTRAINT fk_order_item_fact_menu_dim
    FOREIGN KEY (menu_dim_key) REFERENCES consumption_sch.menu_dim (menu_dim_hk);

ALTER TABLE consumption_sch.order_item_fact ADD CONSTRAINT fk_order_item_fact_delivery_agent_dim
    FOREIGN KEY (delivery_agent_dim_key) REFERENCES consumption_sch.delivery_agent_dim (delivery_agent_hk);

ALTER TABLE consumption_sch.order_item_fact ADD CONSTRAINT fk_order_item_fact_delivery_date_dim
    FOREIGN KEY (order_date_dim_key) REFERENCES consumption_sch.date_dim (date_dim_hk);
-- ---------------------------------------------------------------------------------------------------------------------
-- Use the consumption schema for setting constraints on dimension tables
USE SCHEMA consumption_sch;

-- 1. Customer Dimension
ALTER TABLE consumption_sch.customer_dim
ADD CONSTRAINT pk_customer_dim PRIMARY KEY (customer_hk);

-- 2. Customer Address Dimension
ALTER TABLE consumption_sch.customer_address_dim
ADD CONSTRAINT pk_customer_address_dim PRIMARY KEY (customer_address_hk);

-- 3. Restaurant Dimension
ALTER TABLE consumption_sch.restaurant_dim
ADD CONSTRAINT pk_restaurant_dim PRIMARY KEY (restaurant_hk);

-- 4. Restaurant Location Dimension (This should be the correct one, as SCD1 tables often use the business key as well)
-- Note: Your DDL for restaurant_location_dim already has a PRIMARY KEY on restaurant_location_sk,
-- but the FK references restaurant_location_hk. We must ensure restaurant_location_hk is unique.
-- I'll define it as a UNIQUE key since the surrogate key (SK) is the primary key.
ALTER TABLE consumption_sch.restaurant_location_dim
ADD CONSTRAINT uk_restaurant_location_hk UNIQUE (restaurant_location_hk);

-- 5. Menu Dimension
ALTER TABLE consumption_sch.menu_dim
ADD CONSTRAINT pk_menu_dim PRIMARY KEY (menu_dim_hk);

-- 6. Delivery Agent Dimension
ALTER TABLE consumption_sch.delivery_agent_dim
ADD CONSTRAINT pk_delivery_agent_dim PRIMARY KEY (delivery_agent_hk);

-- 7. Date Dimension (Already defined as PRIMARY KEY in your DDL, but included for completeness)
-- Your DDL for DATE_DIM already includes: DATE_DIM_HK NUMBER PRIMARY KEY
-- and a UNIQUE constraint on CALENDAR_DATE.

-- ----------------------------------------------------------------------
--  IMPORTANT: The DATE_DIM DDL already set the PRIMARY KEY:
-- CREATE OR REPLACE TABLE CONSUMPTION_SCH.DATE_DIM (DATE_DIM_HK NUMBER PRIMARY KEY...);
-- The Date Dimension's Primary Key is already set, so no action is needed here.
-- ----------------------------------------------------------------------


-- 13. Final Views for Analysis (KPIs)
-- ---------------------------------------------------------------------------------------------------------------------

use schema consumption_sch;

CREATE OR REPLACE VIEW consumption_sch.vw_yearly_revenue_kpis AS
SELECT
    d.year AS year,
    SUM(fact.subtotal) AS total_revenue,
    COUNT(DISTINCT fact.order_id) AS total_orders,
    ROUND(SUM(fact.subtotal) / COUNT(DISTINCT fact.order_id), 2) AS avg_revenue_per_order,
    ROUND(SUM(fact.subtotal) / COUNT(fact.order_item_id), 2) AS avg_revenue_per_item,
    MAX(fact.subtotal) AS max_order_value
FROM consumption_sch.order_item_fact fact
JOIN consumption_sch.date_dim d ON fact.order_date_dim_key = d.date_dim_hk
WHERE DELIVERY_STATUS = 'Delivered'
GROUP BY d.year
ORDER BY d.year;


CREATE OR REPLACE VIEW consumption_sch.vw_monthly_revenue_kpis AS
SELECT
    d.YEAR AS year,
    d.MONTH AS month,
    SUM(fact.subtotal) AS total_revenue,
    COUNT(DISTINCT fact.order_id) AS total_orders,
    ROUND(SUM(fact.subtotal) / COUNT(DISTINCT fact.order_id), 2) AS avg_revenue_per_order,
    ROUND(SUM(fact.subtotal) / COUNT(fact.order_item_id), 2) AS avg_revenue_per_item,
    MAX(fact.subtotal) AS max_order_value
FROM consumption_sch.order_item_fact fact
JOIN consumption_sch.DATE_DIM d ON fact.order_date_dim_key = d.DATE_DIM_HK
WHERE DELIVERY_STATUS = 'Delivered'
GROUP BY d.YEAR, d.MONTH
ORDER BY d.YEAR, d.MONTH;


CREATE OR REPLACE VIEW consumption_sch.vw_daily_revenue_kpis AS
SELECT
    d.YEAR AS year,
    d.MONTH AS month,
    d.DAY_OF_THE_MONTH AS day,
    SUM(fact.subtotal) AS total_revenue,
    COUNT(DISTINCT fact.order_id) AS total_orders,
    ROUND(SUM(fact.subtotal) / COUNT(DISTINCT fact.order_id), 2) AS avg_revenue_per_order,
    ROUND(SUM(fact.subtotal) / COUNT(fact.order_item_id), 2) AS avg_revenue_per_item,
    MAX(fact.subtotal) AS max_order_value
FROM consumption_sch.order_item_fact fact
JOIN consumption_sch.DATE_DIM d ON fact.order_date_dim_key = d.DATE_DIM_HK
WHERE DELIVERY_STATUS = 'Delivered'
GROUP BY d.YEAR, d.MONTH, d.DAY_OF_THE_MONTH
ORDER BY d.YEAR, d.MONTH, d.DAY_OF_THE_MONTH;


CREATE OR REPLACE VIEW consumption_sch.vw_day_revenue_kpis AS
SELECT
    d.YEAR AS year,
    d.MONTH AS month,
    d.DAY_NAME AS DAY_NAME,
    SUM(fact.subtotal) AS total_revenue,
    COUNT(DISTINCT fact.order_id) AS total_orders,
    ROUND(SUM(fact.subtotal) / COUNT(DISTINCT fact.order_id), 2) AS avg_revenue_per_order,
    ROUND(SUM(fact.subtotal) / COUNT(fact.order_item_id), 2) AS avg_revenue_per_item,
    MAX(fact.subtotal) AS max_order_value
FROM consumption_sch.order_item_fact fact
JOIN consumption_sch.DATE_DIM d ON fact.order_date_dim_key = d.DATE_DIM_HK
GROUP BY d.YEAR, d.MONTH, d.DAY_NAME
ORDER BY d.YEAR, d.MONTH, d.DAY_NAME;


CREATE OR REPLACE VIEW consumption_sch.vw_monthly_revenue_by_restaurant AS
SELECT
    d.YEAR AS year,
    d.MONTH AS month,
    fact.DELIVERY_STATUS,
    r.name AS restaurant_name,
    SUM(fact.subtotal) AS total_revenue,
    COUNT(DISTINCT fact.order_id) AS total_orders,
    ROUND(SUM(fact.subtotal) / COUNT(DISTINCT fact.order_id), 2) AS avg_revenue_per_order,
    ROUND(SUM(fact.subtotal) / COUNT(fact.order_item_id), 2) AS avg_revenue_per_item,
    MAX(fact.subtotal) AS max_order_value
FROM consumption_sch.order_item_fact fact
JOIN consumption_sch.DATE_DIM d ON fact.order_date_dim_key = d.DATE_DIM_HK
JOIN consumption_sch.restaurant_dim r ON fact.restaurant_dim_key = r.RESTAURANT_HK
GROUP BY d.YEAR, d.MONTH, fact.DELIVERY_STATUS, restaurant_name
ORDER BY d.YEAR, d.MONTH;

-- ---------------------------------------------------------------------------------------------------------------------
